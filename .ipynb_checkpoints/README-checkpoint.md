# Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation

This repository contains the official implementation for the paper: **"Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation"** (NeurIPS 2025).

**Authors:** Sungmin Cha and Kyunghyun Cho

---

### ðŸ“– Abstract

Knowledge distillation (KD) is a core component in the training and deployment of modern generative models, particularly large language models (LLMs). While its empirical benefits are well documented, the underlying mechanisms remain poorly understood.

In this work, we present a **minimal working explanation** of KD in generative modeling. We demonstrate that distillation induces a **trade-off between precision and recall** in the student model.

* **Precision (Quality):** As the teacher becomes more selective (lower entropy), the student concentrates probability mass on high-likelihood regions.
* **Recall (Coverage):** This comes at the expense of distributional coverage.

We validate this effect using controlled Mixture of Gaussians (MoG) simulations and large-scale experiments with the **SmolLM2** family.

---

### Setup

Please install the necessary dependencies before running the experiments.

```bash
pip install -r requirements.txt
```
---

### 1. Mixture of Gaussians (MoG) Experiment

We provide a self-contained Jupyter Notebook to reproduce the toy experiments demonstrating the Precision-Recall trade-off in a controlled environment as described in Section 3 of the paper.

* **File:** `MoG/KD_MoG.ipynb`
* **Usage:** simply run the cells in the notebook sequentially to visualize how teacher entropy affects student distribution.

---

### 2. Large Language Model (LLM) Experiments

We investigate the mechanism in a realistic setting using the **SmolLM2** family. The pipeline consists of:

1. **Teacher:** SmolLM2-360M (p') distilled from SmolLM2-1.7B (p*).
2. **Student:** SmolLM2-135M (p'') distilled from the Teacher (p'; 360M).

The following scripts allow you to reproduce the full pipeline.

#### Figure 1: LLM experiments
![Overview of LLM experiments setup](figs/overview.png)


#### 2.1 Data Generation (`run_generate_data.sh`)

First, we generate training data from the Ground Truth (GT) model and sample data from the Teacher/Student models.

* Generates data from the pretrained **SmolLM2-1.7B (GT)**.
* Samples data from **SmolLM2-360M (p')** and **SmolLM2-135M (p'')** for KD and evaluation.
* **Note:** Validation datasets are sampled independently using the same procedure to ensuring fair evaluation.



```bash
bash run_generate_data.sh
```

#### 2.2 Model Training (`run_train.sh`)

Train the models using either Direct Training (on GT data) or Knowledge Distillation (on Teacher data).

* **Direct Training:** Trains a model directly on samples from the GT (p*).
* **Knowledge Distillation:** Trains the student (p'') on samples generated by the teacher (p') with varying temperature scales to control entropy.


```bash
bash run_train.sh
```

#### 2.3 Model Selection (`run_eval_perplexity.sh`)

We select the best checkpoints based on perplexity measured on the held-out validation set.

* Evaluates perplexity for both Pretrained and Distilled models to identify the optimal checkpoint for the final analysis.

```bash
bash run_eval_perplexity.sh
```

#### 2.4 Evaluation: Precision & Recall (`run_eval_precision_and_recall.sh`)

Finally, we compute the Precision and Recall of the student models against the Ground Truth to verify the trade-off.

* **Precision:**  How well student samples are supported by GT.
* **Recall:**  How well GT samples are covered by the student.



```bash
bash run_eval_precision_and_recall.sh
```

---

### ðŸ”— Citation

If you find this work or code useful, please cite our paper:

```bibtex
@inproceedings{
cha2025why,
title={Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation},
author={Sungmin Cha and Kyunghyun Cho},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=hg5UGeAr1Q}
}

```